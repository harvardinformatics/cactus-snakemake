#############################################################################
# Pipeline for adding a new genome as an outgroup to a HAL file generated 
# by Cactus
#
# Created April 2025
# Gregg Thomas
#############################################################################

import sys
import os
import re
import logging
import subprocess

import lib.cactuslib as CACTUSLIB
from lib.cactuslib import spacedOut as SO
import lib.updatelib as UPDATELIB

from functools import partial

#############################################################################
# System setup

config_flag = config.get("display", False);
version_flag = config.get("version", False);
info_flag = config.get("info", False);
debug = config.get("debug", False);
prep_only = config.get("prep", False);
#debug = True;
# A hacky way to get some custom command line arguments for the pipeline
# These just control preprocessing flags that stop the pipeline early anyways

pad = config.get("pad", 40);
debug_pad = pad - 1;
# The padding for some of the log messages

MAIN, DRY_RUN, OUTPUT_DIR, LOG_DIR, TMPDIR, LOG_LEVEL, LOG_VERBOSITY, TOP_LEVEL_EXECUTOR = CACTUSLIB.pipelineSetup(config, sys.argv, version_flag, info_flag, config_flag, debug, workflow, pad);
# Setup the pipeline, including the output directory, log directory, and tmp directory

CLOG = logging.getLogger('cactuslib')
# Setup logging if debugging

getRuleResources = partial(CACTUSLIB.getResources, config, TOP_LEVEL_EXECUTOR);
# This maps the function to get rule resources from the config file
# so we don't have to pass config each time we call it

#############################################################################
# Cactus setup

USE_GPU = config["use_gpu"]
# Whether to use GPU or CPU cactus

CACTUS_PATH, CACTUS_PATH_TMP, VERSION_TAG = CACTUSLIB.parseCactusPath(config["cactus_path"], USE_GPU, MAIN, TMPDIR, pad);
# Parse the cactus path from the config file

KEG_PATCH_FILE = None
if USE_GPU:
    KEG_PATCH_FILE = CACTUSLIB.downloadKegPatch(OUTPUT_DIR, MAIN, VERSION_TAG)
# Download the KEG patch file if using GPU cactus, and set the path to it

#############################################################################
# Input files and output paths

GENOME_NAME = config["new_genome_name"];
GENOME_FASTA = config["new_genome_fasta"];
GENOME_EXT = os.path.splitext(GENOME_FASTA)[1][1:];
# The name of the new genome to be added to the tree

NEW_GENOME_FILE = os.path.join(OUTPUT_DIR, f"{GENOME_NAME}.{GENOME_EXT}");
# The new genome file to be added to the tree

OLD_ROOT = config["old_root_name"];
OLD_ROOT_FASTA = os.path.join(OUTPUT_DIR, f"{OLD_ROOT}.fa");

NEW_ROOT = config["new_root_name"];
NEW_ROOT_FASTA = os.path.join(OUTPUT_DIR, f"{NEW_ROOT}.fa"); # unused except as placeholder
NEW_ROOT_PAF = os.path.join(OUTPUT_DIR, f"{NEW_ROOT}.paf");
# The name of the old root genome and the new root genome

BL_FROM_NEW = config.get("branch_length_from_new_genome", 1.0);
BL_FROM_OLD = config.get("branch_length_from_old_root", 1.0);
# The new branch length for the new genome

INPUT_HAL = config["input_hal"];
MAF_REFERENCE = config["maf_reference"];
FINAL_PREFIX = config["final_prefix"]
# Other various input parameters

OUTPUT_HAL = os.path.join(OUTPUT_DIR, f"{FINAL_PREFIX}.hal");
OUTPUT_MAF = os.path.join(OUTPUT_DIR, f"{FINAL_PREFIX}.{MAF_REFERENCE}.maf.gz");
OUTPUT_MAF_NODUPES = os.path.join(OUTPUT_DIR, f"{FINAL_PREFIX}.{MAF_REFERENCE}.nodupes.maf.gz");
# The output files generated by the pipeline

if MAIN:
    CLOG.info(SO(f"Output HAL file will be at", pad) + f"{OUTPUT_HAL}");
    CLOG.info(SO(f"Reference genome for MAF file will be", pad) + f"{MAF_REFERENCE}");
    CLOG.info(SO(f"Output MAF file will be at", pad) + f"{OUTPUT_MAF}");
# The final output files for the pipeline

#############################################################################
# Reading files

file_dict = { 'new-genome' : { 'fasta-orig' : os.path.abspath(GENOME_FASTA),
                                'fasta-preprocess' : os.path.abspath(NEW_GENOME_FILE),
                                'name' : GENOME_NAME,
                                'branch-length' : BL_FROM_NEW },
                'old-root' : { 'fasta' : os.path.abspath(OLD_ROOT_FASTA),
                                'name' : OLD_ROOT,
                                'branch-length' : BL_FROM_OLD },
                'new-root' : { 'fasta' : os.path.abspath(NEW_ROOT_FASTA),
                                'name' : NEW_ROOT } 
            };
# Packing up some file info, just makes it easier to pass to the function below

PRE_INPUT_FILE = os.path.join(OUTPUT_DIR, "add-outgroup-pre.txt");
POST_INPUT_FILE = os.path.join(OUTPUT_DIR, "add-outgroup-post.txt");
if MAIN:
    UPDATELIB.createAddOutgroupInputFiles(file_dict, PRE_INPUT_FILE, POST_INPUT_FILE, OUTPUT_DIR, pad);
# Create some of the input files necessary for the pipeline

####################

if MAIN:
    CLOG.info(SO("New genome name", pad) + GENOME_NAME);
    CLOG.info(SO("Input genome fasta", pad) + GENOME_FASTA);
    CLOG.info(SO("Root node from original HAL", pad) + OLD_ROOT);
    CLOG.info(SO("New root node", pad) + NEW_ROOT);
    CLOG.info(SO(f"Branch length from {GENOME_NAME}-{NEW_ROOT}", pad) + str(BL_FROM_NEW));
    CLOG.info(SO(f"Branch length from {OLD_ROOT}-{NEW_ROOT}", pad) + str(BL_FROM_OLD));

if LOG_LEVEL == "debug":
    for node in file_dict:
        CLOG.debug(f"Node: {node}");
        for key in file_dict[node]:
            CLOG.debug(SO(f"{key}", debug_pad) + f"{file_dict[node][key]}");
    CLOG.debug("===================================================================================");
    # The dictionary for storing information and file paths for the nodes in the tree:

####################

if LOG_LEVEL == "debug":
    CLOG.debug("EXITING BEFORE RULES. DEBUG MODE.");
    sys.exit(0);
# Exit before running rules if in debug mode

if prep_only:
    CLOG.info("PREP ONLY FLAG SET. EXITING.");
    sys.exit(0);
# Exit before running rules if prep only flag is set

#############################################################################
# Final rule - rule that depends on final expected output file and initiates all
# the other rules

localrules: all

rule all:
    input:
        final_maf = OUTPUT_MAF,
        final_maf_nodupes = OUTPUT_MAF_NODUPES
## Rule all specifies the final output files expected

# #############################################################################
# # Pipeline rules

rule get_root_fasta:
    input:
        input_hal = INPUT_HAL
    output:
        old_root_fasta = OLD_ROOT_FASTA
    params:
        path = CACTUS_PATH,
        node = OLD_ROOT,
        rule_name = "get_root_fasta"
    log:
        job_log = os.path.join(LOG_DIR, "get_root_fasta.log")
    resources:
        **getRuleResources("get_root_fasta")
    run:
        cmd = params.path + [
            "hal2fasta",
            input.input_hal,
            params.node,
            "--outFaPath", output.old_root_fasta,
            "--hdf5InMemory"
        ];

        CACTUSLIB.runCommand(cmd, None, log.job_log, params.rule_name, OLD_ROOT)

####################

rule preprocess:
    input:
        new_genome_fasta = GENOME_FASTA,
        old_root_fasta = OLD_ROOT_FASTA,
        pre_in  = PRE_INPUT_FILE,
        post_in = POST_INPUT_FILE
    output:
        preprocessed_genome_fasta = NEW_GENOME_FILE
    params:
        path = CACTUS_PATH,
        genome_name = GENOME_NAME,
        job_tmp_dir = os.path.join("/tmp", f"{GENOME_NAME}-preprocess"), # This is the tmp dir in the container, which is bound to the host tmp dir
        host_tmp_dir = os.path.join(TMPDIR, f"{GENOME_NAME}-preprocess"), # This is the tmp dir for the host system, which is bound to /tmp in the singularity container
        rule_name = "preprocess"
    log:
        job_log = os.path.join(LOG_DIR, f"{GENOME_NAME}.{GENOME_EXT}.preprocess.log")
    resources:
        **getRuleResources("preprocess")
    run:
        cmd = params.path + [
            "cactus-preprocess",
            params.job_tmp_dir,
            input.pre_in,
            input.post_in,
            "--inputNames", params.genome_name,
            "--logInfo",
            "--retryCount", "0",
            "--maxCores", str(resources.cpus_per_task)
        ];

        CACTUSLIB.runCommand(cmd, params.host_tmp_dir, log.job_log, params.rule_name, params.genome_name)
        # When not requesting all CPU on a node: toil.batchSystems.abstractBatchSystem.InsufficientSystemResources: The job LastzRepeatMaskJob is requesting 64.0 cores, more than the maximum of 32 cores that SingleMachineBatchSystem was configured with, or enforced by --maxCores.Scale is set to 1.0.
## This rule runs cactus-preprocess for every genome (tip in the tree), which does some masking
## Runtimes for turtles range from 8 to 15 minutes with the above resoureces

####################

rule blast:
    input:
        preprocessed_genome_fasta = NEW_GENOME_FILE,
    output:
        paf_file = NEW_ROOT_PAF
    params:
        path = CACTUS_PATH_TMP, # Note that if USE_GPU is False, this will be the same as CACTUS_PATH_TMP
        node = NEW_ROOT,
        post_in = POST_INPUT_FILE, # Note this has to be a param instead of an input or else it will retrigger each time the pipeline is run (since it is created during preprocessing)
        job_tmp_dir = os.path.join("/tmp", f"{NEW_ROOT}-blast"), # This is the tmp dir in the container, which is bound to the host tmp dir
        host_tmp_dir = os.path.join(TMPDIR, f"{NEW_ROOT}-blast"), # This is the tmp dir for the host system, which is bound to /tmp in the singularity container
        gpu_opt = USE_GPU,
        gpu_num = config['rule_resources']['blast']['gpus'],
        rule_name = "blast"
    log:
        job_log = os.path.join(LOG_DIR, f"{NEW_ROOT}.blast.log")
    resources:
        **getRuleResources("blast"),
        slurm_extra = f"'--gres=gpu:{config['rule_resources']['blast']['gpus']}'" if USE_GPU else ""
    run:
        cmd = params.path + [
            "cactus-blast",
            params.job_tmp_dir,
            params.post_in,
            output.paf_file,
            "--root", params.node,
            "--logInfo",
            "--retryCount", "0",
            "--lastzCores", str(resources.cpus_per_task)
        ];

        if params.gpu_opt:
            cmd += ["--gpu", str(params.gpu_num)];

        CACTUSLIB.runCommand(cmd, params.host_tmp_dir, log.job_log, params.rule_name, params.node)
## This rule runs cactus-blast for every internal node
## Runtimes for turtles range from 1 to 10 hours with the above resources

####################

rule align:
    input:
        paf_file = NEW_ROOT_PAF
    output:
        hal_file = OUTPUT_HAL
    params:
        path = CACTUS_PATH_TMP,
        post_in = POST_INPUT_FILE, # Note this has to be a param instead of an input or else it will retrigger each time the pipeline is run (since it is created during preprocessing)
        node = NEW_ROOT,
        keg_patch_file = KEG_PATCH_FILE,
        job_tmp_dir = os.path.join("/tmp", f"{NEW_ROOT}-align"), # This is the tmp dir in the container, which is bound to the host tmp dir
        host_tmp_dir = os.path.join(TMPDIR, f"{NEW_ROOT}-align"), # This is the tmp dir for the host system, which is bound to /tmp in the singularity container
        work_dir = TMPDIR,
        rule_name = "align"
    log:
        job_log = os.path.join(LOG_DIR, f"{NEW_ROOT}.align.log")
    resources:
        **getRuleResources("align")
    run:
        cmd = params.path + [
            "cactus-align",
            params.job_tmp_dir,
            params.post_in,
            input.paf_file,
            output.hal_file,
            "--root", params.node,
            "--logInfo",
            "--retryCount", "0",
            "--maxCores", str(resources.cpus_per_task),
        ];

        if params.keg_patch_file:
            cmd += ["--configFile", params.keg_patch_file];

        CACTUSLIB.runCommand(cmd, params.host_tmp_dir, log.job_log, params.rule_name, params.node)
## This rule runs cactus-align for every internal node
## Runtimes for turtles range from 4 to 16 hours with the above resources

####################

rule combine_hals:
    input:
        original_hal = INPUT_HAL,
        new_hal = OUTPUT_HAL
    output:
        combine_hals_stamp = os.path.join(LOG_DIR, "combine-hals.stamp")
    params:
        path = CACTUS_PATH,
        node = OLD_ROOT,
        job_tmp_dir = os.path.join("/tmp", "combine-hals"), # This is the tmp dir in the container, which is bound to the host tmp dir
        host_tmp_dir = os.path.join(TMPDIR, "combine-hals"), # This is the tmp dir for the host system, which is bound to /tmp in the singularity container
        rule_name = "combine_hals"
    log:
        job_log = os.path.join(LOG_DIR, "combine-hals.log")
    resources:
        **getRuleResources("combine_hals")
    run:
        cmd = params.path + [
            "halAppendSubtree",
            input.new_hal,
            input.original_hal,
            params.node,
            params.node,
            "--merge",
            "--hdf5InMemory"
        ];

        CACTUSLIB.runCommand(cmd, params.host_tmp_dir, log.job_log, params.rule_name, "combine_hals")

        with open(output.combine_hals_stamp, "w") as f:
            f.write("done");


####################

rule maf:
    input:
        combine_hals_stamp = os.path.join(LOG_DIR, "combine-hals.stamp")
    output:
        final_maf = OUTPUT_MAF,
        final_maf_nodupes = OUTPUT_MAF_NODUPES
    params:
        path = CACTUS_PATH_TMP,
        new_hal = OUTPUT_HAL,
        ref_genome = MAF_REFERENCE,
        chunk_size = 500000, # 500kb
        host_tmp_dir = os.path.join(TMPDIR, "maf"),
        job_tmp_dir = os.path.join("/tmp", "maf"),
        rule_name = "maf"
    log:
        job_log = os.path.join(LOG_DIR, "maf.log")
    resources:
        **getRuleResources("maf")
    run:
        cmd = params.path + [
            "cactus-hal2maf",
            params.job_tmp_dir,
            params.new_hal,
            output.final_maf,
            "--refGenome", params.ref_genome,
            "--chunkSize", str(params.chunk_size),
            "--batchCount", str(resources.cpus_per_task),
            "--filterGapCausingDupes"
        ];

        CACTUSLIB.runCommand(cmd, params.job_tmp_dir, log.job_log, params.rule_name);

        cmd = params.path + [
            "cactus-hal2maf",
            params.job_tmp_dir,
            params.new_hal,
            output.final_maf_nodupes,
            "--refGenome", params.ref_genome,
            "--chunkSize", str(params.chunk_size),
            "--batchCount", str(resources.cpus_per_task),
            "--filterGapCausingDupes",
            "--dupeMode", "single"
        ];

        CACTUSLIB.runCommand(cmd, params.host_tmp_dir, log.job_log, params.rule_name, fmode="a+");

#############################################################################
