#############################################################################
# Pipeline for running cactus for whole genome alignment
#############################################################################

import sys
import os
import re
import logging
import subprocess

import lib.cactuslib as cactuslib
import lib.treelib as treelib

#############################################################################
# System setup

MAIN = True;
if "__main__.py" in sys.argv[0]:
    MAIN = False;
# Whether the pipeline is being run as a main script or not

DRY_RUN = False;
if any([arg in sys.argv for arg in ["--dry-run", "--dryrun", "-n"]]):
    DRY_RUN = True;
# Whether the pipeline is running in dry-run mode

log_level = "info";

if any([arg in sys.argv for arg in ["--rulegraph", "--dag"]]):
    log_level = "notset";
# Set the log level based on the arguments

#log_level = "debug";
# Uncomment to set the log level to debug

OUTPUT_DIR = os.path.abspath(config["output_dir"]);
LOG_DIR = os.path.join(OUTPUT_DIR, "logs");
# The output directory where all the files and logs are stored

if MAIN:
    outdir_log_msg, outdir_err_flag = cactuslib.createOutputDirs(OUTPUT_DIR, LOG_DIR, config["overwrite_output_dir"], DRY_RUN);
# Create the output directories if they don't exist

log_verbosity = "both"; # "screen", "file", "both"
if DRY_RUN:
    log_verbosity = "screen";
# Set the log verbosity based on the arguments

log_filename = os.path.join(LOG_DIR, f"cactus-snakemake.{log_level}.log"); # Log file name if log_verbosity is "file" or "both"
cactuslib.configureLogging(log_filename, log_level.upper(), log_verbosity.upper())
cactuslib_logger = logging.getLogger('cactuslib')
# Setup logging if debugging

if MAIN:
    cactuslib_logger.info(f"MAIN call: {' '.join(sys.argv)}");
    # Log the command that was run to start the pipeline

    if outdir_err_flag:
        cactuslib_logger.error(outdir_log_msg);
        sys.exit(1);
    else:
        cactuslib_logger.info(outdir_log_msg);
    # Log the command that was run to create the output directory
    target_jobs = None;
else:
    target_jobs = None
    if "--target-jobs" in sys.argv:
        target_jobs_index = sys.argv.index("--target-jobs")
        if target_jobs_index + 1 < len(sys.argv):
            target_jobs = sys.argv[target_jobs_index + 1];
            if target_jobs[-1] == ":":
                target_jobs = target_jobs[:-1];
    cactuslib_logger.info(f"RULE {target_jobs} call: {' '.join(sys.argv)}");
    # Log the command that was run for a rule

# wd = config["working_dir"];
# # if not os.path.exists(wd):
# #     cactuslib_logger.info(f"Creating working directory at {wd}");
# #     os.makedirs(wd);
# cactuslib_logger.info(f"Working directory: {os.getcwd()}");
# cactuslib_logger.info(f"Changing working directory to: {wd}");
# os.chdir(wd);
# Switching to the working directory of the project so paths can be relative

USE_GPU = config["use_gpu"]
# Whether to use GPU or CPU cactus

TMPDIR = config["tmp_dir"];
if not os.path.exists(TMPDIR):
    if MAIN:
        cactuslib_logger.info(f"Creating temporary directory at {TMPDIR}");
    os.makedirs(TMPDIR);
# A directory with lots of space to use for temporary files generated by the cactus-align command

if config["cactus_path"].lower() in ["download", ""]:
    cactus_image_path = cactuslib.downloadCactusImage(USE_GPU, MAIN);
else:
    cactus_image_path = config["cactus_path"];
    # The path to the cactus image, either downloaded or specified in the config file

    if not os.path.exists(cactus_image_path):
        cactuslib_logger.error(f"Could not find cactus image at {cactus_image_path}");
        sys.exit(1);
    # Check if the cactus image exists

CACTUS_PATH = ["singularity", "exec", "--nv", "--cleanenv", cactus_image_path]
CACTUS_PATH_TMP = ["singularity", "exec", "--nv", "--cleanenv", "--bind", TMPDIR + ":/tmp", cactus_image_path]
#CACTUS_PATH = "singularity exec --nv --cleanenv " + cactus_image_path
#CACTUS_PATH_TMP = "singularity exec --nv --cleanenv --bind " + TMPDIR + ":/tmp " + cactus_image_path
# The path to the cactus image with and without a tmpdir binding

#############################################################################
# Input files and output paths

INPUT_FILE = os.path.abspath(config["input_file"]);
if not os.path.isfile(INPUT_FILE):
    cactuslib_logger.error(f"Could not find input file at {INPUT_FILE}");
    sys.exit(1);
else:
    if MAIN:
        cactuslib_logger.info(f"Input file found at {INPUT_FILE}");
# The cactus input file used to generate the config file with cactus-prepare

MAF_REFERENCE = config["maf_reference"];

OUTPUT_HAL = os.path.join(OUTPUT_DIR, f"{config["final_prefix"]}.hal");
OUTPUT_MAF = os.path.join(OUTPUT_DIR, f"{config["final_prefix"]}.{MAF_REFERENCE}.maf.gz");
OUTPUT_MAF_NODUPES = os.path.join(OUTPUT_DIR, f"{config["final_prefix"]}.{MAF_REFERENCE}.nodupes.maf.gz");

if MAIN:
    cactuslib_logger.info(f"Output HAL file will be at {OUTPUT_HAL}");
    cactuslib_logger.info(f"Reference genome for MAF file will be {MAF_REFERENCE}");
    cactuslib_logger.info(f"Output MAF file will be at {OUTPUT_MAF}");
# The final output files for the pipeline

#job_path = os.path.join(OUTPUT_DIR, "jobstore");
# The temporary/job directory specified in cactus-prepare

#############################################################################
# cactus-prepare

if MAIN:
    cactuslib.runCactusPrepare(INPUT_FILE, CACTUS_PATH, OUTPUT_DIR, OUTPUT_HAL, USE_GPU, LOG_DIR, DRY_RUN);
if DRY_RUN:
    CACTUS_FILE = os.path.join("/tmp/", "cactus-smk-dryrun", os.path.basename(INPUT_FILE));
else:
    CACTUS_FILE = os.path.join(OUTPUT_DIR, os.path.basename(INPUT_FILE));
# Run cactus-prepare to generate the cactus input file with ancestral nodes and labeled tree

#############################################################################
# Reading files

tips = cactuslib.readTips(INPUT_FILE, MAIN);
# The main dictionary for storing information and file paths for tips in the tree:
# [output fasta file from preprocess step] : { 'input' : "original genome fasta file", 'name' : "genome name in tree", 'output' : "expected output from preprocess step (same as key)" }

####################

internals, anc_tree = cactuslib.initializeInternals(CACTUS_FILE, tips, MAIN);
# The main dictionary for storing information and file paths for internal nodes in the tree:
# [node name] : { 'name' : "node name in tree", 'blast-inputs' : [the expected inputs for the blast step], 'align-inputs' : [the expected inputs for the align step],
#                   'hal-inputs' : [the expected inputs for the hal2fasta step], 'blast-output' : "the .cigar file output from the blast step",
#                   'align-output' : "the .hal file output from the align step", 'hal-output' : "the fasta file output from the hal2fasta step" }

####################

tinfo, anc_tree, root = treelib.treeParse(anc_tree);
ROOT_NAME = tinfo[root][3];
internals = cactuslib.parseInternals(internals, tips, tinfo, anc_tree);
# The tree is parsed to get the root node and the internal nodes are updated with the correct names

if DRY_RUN:
    import shutil
    shutil.rmtree("/tmp/cactus-smk-dryrun/", ignore_errors=True)
# Remove the dryrun directory if it was created

if log_level == "debug":
    cactuslib_logger.debug("EXITING BEFORE RULES. DEBUG MODE.");
    sys.exit(0);
# Exit before running rules if in debug mode

#############################################################################
# Final rule - rule that depends on final expected output file and initiates all
# the other rules

localrules: all

rule all:
    input:
        final_maf = OUTPUT_MAF,
        final_maf_nodupes = OUTPUT_MAF_NODUPES
        # The .maf file from rul maf
## Rule all specifies the final output files expected

# #############################################################################
# # Pipeline rules

rule preprocess:
    input:
        lambda wildcards: [ tips[name]['input'] for name in tips if tips[name]['output'] == wildcards.final_tip ][0]
    output:
        os.path.join(OUTPUT_DIR, "{final_tip}")    
    params:
        path = CACTUS_PATH,
        input_file = INPUT_FILE,
        cactus_file = os.path.join(OUTPUT_DIR, CACTUS_FILE),
        genome_name = lambda wildcards: [ name for name in tips if tips[name]['output'] == wildcards.final_tip ][0],
        host_tmp_dir = lambda wildcards: os.path.join(TMPDIR, [ name for name in tips if tips[name]['output'] == wildcards.final_tip ][0] + "-preprocess"), # This is the tmp dir for the host system, which is bound to /tmp in the singularity container
        job_tmp_dir = lambda wildcards: os.path.join("/tmp", [ name for name in tips if tips[name]['output'] == wildcards.final_tip ][0] + "-preprocess"), # This is the tmp dir in the container, which is bound to the host tmp dir
        gpu_opt = f"--gpu {config["preprocess_gpu"]}" if USE_GPU else "",
        rule_name = "preprocess"
    log:
        job_log = os.path.join(LOG_DIR, "{final_tip}.preprocess.log")
    resources:
        slurm_partition = config["preprocess_partition"],
        cpus_per_task = config["preprocess_cpu"],
        mem_mb = config["preprocess_mem"],
        runtime = config["preprocess_time"],
        slurm_extra = f"'--gres=gpu:{config["preprocess_gpu"]}'" if USE_GPU else ""
    run:
        cmd = params.path + [
            "cactus-preprocess",
            params.job_tmp_dir,
            params.input_file,
            params.cactus_file,
            "--inputNames",
            params.genome_name,
            "--logInfo",
            "--retryCount", "0",
            "--maxCores", str(resources.cpus_per_task)
        ];

        if params.gpu_opt:
            cmd.append("--gpu");

        cactuslib.runCommand(cmd, params.job_tmp_dir, log.job_log, params.rule_name, wildcards.final_tip)
        # When not requesting all CPU on a node: toil.batchSystems.abstractBatchSystem.InsufficientSystemResources: The job LastzRepeatMaskJob is requesting 64.0 cores, more than the maximum of 32 cores that SingleMachineBatchSystem was configured with, or enforced by --maxCores.Scale is set to 1.0.
    # shell:
    #     """
    #     {params.path} cactus-preprocess {params.job_dir} {params.input_file} {params.cactus_file} --inputNames {params.genome_name} --realTimeLogging true --logInfo --retryCount 0 --maxCores {resources.cpus_per_task} {params.gpu_opt}
    #     """
    # Keeping shell around now for debugging purposes

## This rule runs cactus-preprocess for every genome (tip in the tree), which does some masking
## Runtimes for turtles range from 8 to 15 minutes with the above resoureces

####################

rule blast:
    input:
        lambda wildcards: [ os.path.join(OUTPUT_DIR, input_file) for input_file in internals[wildcards.internal_node]['input-seqs'] ]
    output:
        paf_file = os.path.join(OUTPUT_DIR, "{internal_node}.paf")
    params:
        path = CACTUS_PATH_TMP,
        cactus_file = os.path.join(OUTPUT_DIR, CACTUS_FILE),
        node = lambda wildcards: wildcards.internal_node,
        host_tmp_dir = lambda wildcards: os.path.join(TMPDIR, wildcards.internal_node + "-blast"), # This is the tmp dir for the host system, which is bound to /tmp in the singularity container
        job_tmp_dir = lambda wildcards: os.path.join("/tmp", wildcards.internal_node + "-blast"), # This is the tmp dir in the container, which is bound to the host tmp dir
        gpu_opt = f"--gpu {config["blast_gpu"]}" if USE_GPU else "",
        rule_name = "blast"
    log:
        job_log = os.path.join(LOG_DIR, "{internal_node}.blast.log")
    resources:
        slurm_partition = config["blast_partition"],
        cpus_per_task = config["blast_cpu"],
        mem_mb = config["blast_mem"],
        runtime = config["blast_time"],
        slurm_extra = f"'--gres=gpu:{config["blast_gpu"]}'" if USE_GPU else ""
    run:
        cmd = params.path + [
            "cactus-blast",
            params.job_tmp_dir,
            params.cactus_file,
            output.paf_file,
            "--root", params.node,
            "--logInfo",
            "--retryCount", "0",
            "--lastzCores", str(resources.cpus_per_task)
        ];

        if params.gpu_opt:
            cmd.append("--gpu");
            cmd.append("1");
            # TODO: fix this

        cactuslib.runCommand(cmd, params.job_tmp_dir, log.job_log, params.rule_name, wildcards.internal_node)
    # shell:
    #     """
    #     {params.path} cactus-blast {params.job_tmp_dir} {params.cactus_file} {output} --root {params.node} --logInfo --retryCount 0 --lastzCores {resources.cpus_per_task} {params.gpu_opt}
    #     """
    # Keeping shell around now for debugging purposes
## This rule runs cactus-blast for every internal node
## Runtimes for turtles range from 1 to 10 hours with the above resources

####################

rule align:
    input:
        cigar_file = os.path.join(OUTPUT_DIR, "{internal_node}.paf"),
        #seq_files = lambda wildcards: [ os.path.join(OUTPUT_DIR, input_file) for input_file in internals[wildcards.internal_node]['desc-seqs'] ]
    output:
        hal_file = os.path.join(OUTPUT_DIR, "{internal_node}.hal")
    params:
        path = CACTUS_PATH_TMP,
        #config_file = os.path.join(OUTPUT_DIR, CONFIG_FILE),
        cactus_file = os.path.join(OUTPUT_DIR, CACTUS_FILE),
        node = lambda wildcards: wildcards.internal_node,
        #job_dir = lambda wildcards: os.path.join(TMPDIR, wildcards.internal_node + "-align"),
        host_tmp_dir = lambda wildcards: os.path.join(TMPDIR, wildcards.internal_node + "-align"), # This is the tmp dir for the host system, which is bound to /tmp in the singularity container
        job_tmp_dir = lambda wildcards: os.path.join("/tmp", wildcards.internal_node + "-align"), # This is the tmp dir in the container, which is bound to the host tmp dir
        work_dir = TMPDIR,
        gpu_opt = "--gpu" if USE_GPU else "",
        rule_name = "align"
    log:
        job_log = os.path.join(LOG_DIR, "{internal_node}.align.log")
    resources:
        slurm_partition = config["align_partition"],
        cpus_per_task = config["align_cpu"],
        mem_mb = config["align_mem"],
        runtime = config["align_time"],
        slurm_extra = f"'--gres=gpu:{config["align_gpu"]}'" if USE_GPU else ""
    run:
        cmd = params.path + [
            "cactus-align",
            params.job_tmp_dir,
            params.cactus_file,
            input.cigar_file,
            output.hal_file,
            "--root", params.node,
            "--logInfo",
            "--retryCount", "0",
            "--workDir", params.work_dir,
            "--maxCores", str(resources.cpus_per_task),
            #"--defaultDisk", "450G"
        ];

        if params.gpu_opt:
            cmd.append("--gpu");

        cactuslib.runCommand(cmd, params.job_tmp_dir, log.job_log, params.rule_name, wildcards.internal_node)
    # shell:
    #     """   
    #     {params.path} cactus-align {params.job_tmp_dir} {params.cactus_file} {input.cigar_file} {output} --root {params.node} --logInfo --retryCount 0 --workDir {params.work_dir} --maxCores {resources.cpus_per_task} --defaultDisk 450G {params.gpu_opt}
    #     """
    # Keeping shell around now for debugging purposes
## This rule runs cactus-align for every internal node
## Runtimes for turtles range from 4 to 16 hours with the above resources

####################

rule convert:
    input:
        hal_file = os.path.join(OUTPUT_DIR, "{internal_node}.hal")
        #lambda wildcards: [ os.path.join(output_dir, input_file) for input_file in internals[wildcards.internal_node]['hal-inputs'] ][0]
    output:
        fa_file = os.path.join(OUTPUT_DIR, "{internal_node}.fa")
    params:
        path = CACTUS_PATH,
        node = lambda wildcards: wildcards.internal_node,
        rule_name = "convert"
    log:
        job_log = os.path.join(LOG_DIR, "{internal_node}.convert.log")
    resources:
        slurm_partition = config["convert_partition"],
        cpus_per_task = config["convert_cpu"],
        mem_mb = config["convert_mem"],
        time = config["convert_time"]
    run:
        cmd = params.path + [
            "hal2fasta",
            input.hal_file,
            params.node,
            "--outFaPath", output.fa_file,
            "--hdf5InMemory"
        ];

        cactuslib.runCommand(cmd, None, log.job_log, params.rule_name, params.node)
## This rule runs hal2fasta to convert .hal files for each internal node to .fasta files
## Runtime for turtles is only about 30 seconds per node

####################

rule copy_hal:
    input:
        all_hals = expand(os.path.join(OUTPUT_DIR, "{internal_node}.fa"), internal_node=internals),
        anc_hal = os.path.join(OUTPUT_DIR, ROOT_NAME + ".hal")
    output:
        final_hal = OUTPUT_HAL
    params:
        rule_name = "copy_hal"
    log:
        job_log = os.path.join(LOG_DIR, "copy-hal.log")
    resources:
        slurm_partition = config["copy_partition"],
        cpus_per_task = config["copy_cpu"],
        mem_mb = config["copy_mem"],
        runtime = config["copy_time"]    
    run:
        cmd = ["cp", input.anc_hal, output.final_hal];

        cactuslib.runCommand(cmd, None, log.job_log, params.rule_name)
## Copying the root .hal file here, since failures in the subsequent rules
## would mean the blast/align steps have to be re-run for that node, but this means a little extra
## storage is required

####################

rule append:
    input:
        final_hal = OUTPUT_HAL
    output:
        append_done = touch(os.path.join(LOG_DIR, "hal-append-subtree.done"))
    params:
        path = CACTUS_PATH_TMP,
        job_tmp_dir = os.path.join(TMPDIR, "append-hal"),
        rule_name = "append"
    log:
        job_log = os.path.join(LOG_DIR, "hal-append-subtree.log")
    resources:
        slurm_partition = config["append_partition"],
        cpus_per_task = config["append_cpu"],
        mem_mb = config["append_mem"],
        runtime = config["append_time"]
    run:
        node_count = 1;
        for node in internals:
            if node == ROOT_NAME:
                continue;
            # If the node is the root we don't want to append since that is the hal file we
            # are appending to

            node_hal = os.path.join(OUTPUT_DIR, node + ".hal");

            cmd = params.path + [
                "halAppendSubtree",
                OUTPUT_HAL,
                node_hal,
                node,
                node,
                "--merge",
                "--hdf5InMemory"
            ];

            if node_count == 1:
                file_mode = "w+";
            else:
                file_mode = "a+";

            cactuslib.runCommand(cmd, params.job_tmp_dir, log.job_log, params.rule_name, node, fmode=file_mode);
            # Generate the command for the current node

            node_count += 1;
            # Increment the node count
        ## End node loop
    ## This rule runs halAppendSubtree on every internal node in the tree to combine alignments into a single file.
    ## Because this command writes to the same file for every node, jobs must be run serially.
####################

rule maf:
    input:
        final_hal = OUTPUT_HAL,
        append_done = os.path.join(LOG_DIR, "hal-append-subtree.done")
    output:
        final_maf = OUTPUT_MAF,
        final_maf_nodupes = OUTPUT_MAF_NODUPES
    params:
        path = CACTUS_PATH_TMP,
        ref_genome = MAF_REFERENCE,
        chunk_size = 500000, # 500kb
        job_tmp_dir = os.path.join(TMPDIR, "maf"),
        rule_name = "maf"
    log:
        job_log = os.path.join(LOG_DIR, "maf.log")
    resources:
        slurm_partition = config["maf_partition"],
        cpus_per_task = config["maf_cpu"],
        mem_mb = config["maf_mem"],
        runtime = config["maf_time"]
    run:
        cmd = params.path + [
            "cactus-hal2maf",
            params.job_tmp_dir,
            input.final_hal,
            output.final_maf,
            "--refGenome", params.ref_genome,
            "--chunkSize", str(params.chunk_size),
            "--batchCount", str(resources.cpus_per_task),
            "--filterGapCausingDupes"
        ];

        cactuslib.runCommand(cmd, params.job_tmp_dir, log.job_log, params.rule_name);

        cmd = params.path + [
            "cactus-hal2maf",
            params.job_tmp_dir,
            input.final_hal,
            output.final_maf_nodupes,
            "--refGenome", params.ref_genome,
            "--chunkSize", str(params.chunk_size),
            "--batchCount", str(resources.cpus_per_task),
            "--filterGapCausingDupes",
            "--dupeMode", "single"
        ];

        cactuslib.runCommand(cmd, params.job_tmp_dir, log.job_log, params.rule_name, fmode="a+");

#############################################################################
