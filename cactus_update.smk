#############################################################################
# Pipeline for running cactus for whole genome alignment
#############################################################################

import sys
import os
import re
import logging
import subprocess

import lib.cactuslib as cactuslib
import lib.treelib as treelib

#############################################################################
# System setup

#debug = True;
MAIN, DRY_RUN, OUTPUT_DIR, LOG_DIR, TMPDIR, LOG_LEVEL, LOG_VERBOSITY = cactuslib.pipelineSetup(config, sys.argv, globals().get("debug", False));
# Setup the pipeline, including the output directory, log directory, and tmp directory

cactuslib_logger = logging.getLogger('cactuslib')
# Setup logging if debugging

#############################################################################
# Cactus setup

USE_GPU = config["use_gpu"]
# Whether to use GPU or CPU cactus

if config["cactus_path"] == None or config["cactus_path"].lower() in ["download", ""]:
    cactus_image_path = cactuslib.downloadCactusImage(False, MAIN);
    # Download the cactus image if it is not specified in the config file
else:
    cactus_image_path = config["cactus_path"];
    # The path to the cactus image, either downloaded or specified in the config file

    if not os.path.exists(cactus_image_path):
        cactuslib_logger.error(f"Could not find cactus image at {cactus_image_path}");
        sys.exit(1);
    # Check if the cactus image exists

if USE_GPU:
    if config["cactus_gpu_path"] == None or config["cactus_gpu_path"].lower() in ["download", ""]:
        cactus_gpu_image_path = cactuslib.downloadCactusImage(True, MAIN);
        # Download the cactus GPU image if it is not specified in the config file
    else:
        cactus_gpu_image_path = config["cactus_gpu_path"];
        # The path to the cactus GPU image, either downloaded or specified in the config file

        if not os.path.exists(cactus_gpu_image_path):
            cactuslib_logger.error(f"Could not find cactus GPU image at {cactus_gpu_image_path}");
            sys.exit(1);
        # Check if the cactus GPU image exists
else:
    cactus_gpu_image_path = cactus_image_path;
# If not using GPU, set the cactus GPU image path to the cactus image path

CACTUS_PATH = ["singularity", "exec", "--cleanenv", cactus_image_path]
CACTUS_PATH_TMP = ["singularity", "exec", "--cleanenv", "--bind", TMPDIR + ":/tmp", cactus_image_path]

CACTUS_GPU_PATH = ["singularity", "exec", "--nv", "--cleanenv", cactus_gpu_image_path]
CACTUS_GPU_PATH_TMP = ["singularity", "exec", "--nv", "--cleanenv", "--bind", TMPDIR + ":/tmp", cactus_gpu_image_path]
# The path to the cactus image with and without a tmpdir binding

#############################################################################
# Input files and output paths

INPUT_FILE = os.path.abspath(config["input_file"]);
if not os.path.isfile(INPUT_FILE):
    cactuslib_logger.error(f"Could not find input file at {INPUT_FILE}");
    sys.exit(1);
else:
    if MAIN:
        cactuslib_logger.info(f"Input file found at {INPUT_FILE}");
# The cactus input file used to generate the config file with cactus-prepare

INPUT_HAL = config["input_hal"];
MAF_REFERENCE = config["maf_reference"];
FINAL_PREFIX = config["final_prefix"]
# Various input parameters

if config["overwrite_original_hal"]:
    HAL_TO_EDIT = os.path.abspath(INPUT_HAL);
else:
    HAL_TO_EDIT = os.path.join(OUTPUT_DIR, f"{FINAL_PREFIX}.hal");
# The output HAL file generated by the pipeline

OUTPUT_MAF = os.path.join(OUTPUT_DIR, f"{FINAL_PREFIX}.{MAF_REFERENCE}.maf.gz");
OUTPUT_MAF_NODUPES = os.path.join(OUTPUT_DIR, f"{FINAL_PREFIX}.{MAF_REFERENCE}.nodupes.maf.gz");
# The output MAF files generated by the pipeline

if MAIN:
    cactuslib_logger.info(f"Output HAL file will be at {HAL_TO_EDIT}");
    cactuslib_logger.info(f"Reference genome for MAF file will be {MAF_REFERENCE}");
    cactuslib_logger.info(f"Output MAF file will be at {OUTPUT_MAF}");
# The final output files for the pipeline

UPDATE_TYPE = "branch";
PARENT  = config["parent_node"];
CHILD   = config["child_node"];
ANCNAME = config["anc_name"];
ORIG_BL = str(config["orig_branch_length"]); # NOTE: Could read tree to get this
TOP_BL  = str(config["top_branch_length"]);
# The update type and the parent and child genomes for the update

#############################################################################
# cactus-prepare

if MAIN:
    SEQ_FILES = cactuslib.runCactusUpdatePrepare(INPUT_HAL, INPUT_FILE, CACTUS_PATH, OUTPUT_DIR, UPDATE_TYPE, USE_GPU, LOG_DIR, DRY_RUN, PARENT, CHILD, ANCNAME, TOP_BL);
# if DRY_RUN:
#     CACTUS_FILE = os.path.join("/tmp/", "cactus-update-smk-dryrun", os.path.basename(INPUT_FILE));
# else:
CACTUS_FILE = os.path.join(OUTPUT_DIR, os.path.basename(INPUT_FILE));
# Run cactus-prepare to generate the cactus input file with ancestral nodes and labeled tree

#############################################################################
# Reading files

GENOME_NAME, GENOME_EXT = cactuslib.getGenomesToAdd(INPUT_FILE);
# Get the genomes to add from the input file

NEW_GENOME_FILE = os.path.join(OUTPUT_DIR, f"{GENOME_NAME}.{GENOME_EXT}");

rounds = { ANCNAME : { "name" : ANCNAME, 
                            "blast-input" : NEW_GENOME_FILE,
                            "blast-output" : os.path.join(OUTPUT_DIR, ANCNAME + ".paf"), 
                            "align-output" : os.path.join(OUTPUT_DIR, ANCNAME + ".hal"),
                            "convert-output" : os.path.join(OUTPUT_DIR, ANCNAME + ".fa") },
            PARENT : { "name" : PARENT, 
                        "blast-input" : [os.path.join(OUTPUT_DIR, ANCNAME + ".hal"), os.path.join(OUTPUT_DIR, ANCNAME + ".fa")], 
                        # This technically also needs the other descendant seq file, but it should always be there
                        # Would be better to parse the tree for these, but not necessary at this point...

                        "blast-output" : os.path.join(OUTPUT_DIR, PARENT + ".paf"),
                        "align-output" : os.path.join(OUTPUT_DIR, PARENT + ".hal"),
                        "convert-output" : os.path.join(OUTPUT_DIR, PARENT + ".fa") }, 
        };

if LOG_LEVEL == "debug":
    for node in rounds:
        cactuslib_logger.debug(f"Node: {node}");
        for key in rounds[node]:
            cactuslib_logger.debug(f"{key}: {rounds[node][key]}");
    cactuslib_logger.debug("===================================================================================");
    # The dictionary for storing information and file paths for the nodes in the tree:

####################

if LOG_LEVEL == "debug":
    cactuslib_logger.debug("EXITING BEFORE RULES. DEBUG MODE.");
    sys.exit(0);
# Exit before running rules if in debug mode

#############################################################################
# Final rule - rule that depends on final expected output file and initiates all
# the other rules

# wildcard_constraints:
#     node = f"^(?!{FINAL_PREFIX}$).*"

localrules: all

rule all:
    input:
        final_maf = OUTPUT_MAF,
        final_maf_nodupes = OUTPUT_MAF_NODUPES
## Rule all specifies the final output files expected

# #############################################################################
# # Pipeline rules

rule preprocess:
    input:
        seq_in  = os.path.join(OUTPUT_DIR, "seq_file.in"),
        seq_out = os.path.join(OUTPUT_DIR, "seq_file.out"),
    output:
        NEW_GENOME_FILE
    params:
        path = CACTUS_PATH,
        genome_name = GENOME_NAME,
        job_tmp_dir = os.path.join("/tmp", f"{GENOME_NAME}-preprocess"), # This is the tmp dir in the container, which is bound to the host tmp dir
        host_tmp_dir = os.path.join(TMPDIR, f"{GENOME_NAME}-preprocess"), # This is the tmp dir for the host system, which is bound to /tmp in the singularity container
        rule_name = "preprocess"
    log:
        job_log = os.path.join(LOG_DIR, f"{GENOME_NAME}.{GENOME_EXT}.preprocess.log")
    resources:
        slurm_partition = config["preprocess_partition"],
        cpus_per_task = config["preprocess_cpu"],
        mem_mb = config["preprocess_mem"],
        runtime = config["preprocess_time"],
        # slurm_extra = f"'--gres=gpu:{config["preprocess_gpu"]}'" if USE_GPU else ""
    run:
        cmd = params.path + [
            "cactus-preprocess",
            params.job_tmp_dir,
            input.seq_in,
            input.seq_out,
            "--inputNames", params.genome_name,
            "--logInfo",
            "--retryCount", "0",
            "--maxCores", str(resources.cpus_per_task)
        ];

        # if params.gpu_opt:
        #     cmd.append("--gpu");

        cactuslib.runCommand(cmd, params.host_tmp_dir, log.job_log, params.rule_name, params.genome_name)
        # When not requesting all CPU on a node: toil.batchSystems.abstractBatchSystem.InsufficientSystemResources: The job LastzRepeatMaskJob is requesting 64.0 cores, more than the maximum of 32 cores that SingleMachineBatchSystem was configured with, or enforced by --maxCores.Scale is set to 1.0.
## This rule runs cactus-preprocess for every genome (tip in the tree), which does some masking
## Runtimes for turtles range from 8 to 15 minutes with the above resoureces

####################

rule blast:
    input:
        lambda wildcards: rounds[wildcards.node]['blast-input']
        #lambda wildcards: [ rounds[name]['blast-input'] for name in rounds if name == wildcards.node ]
        #expand(os.path.join(OUTPUT_DIR, "{genome_name}.{genome_ext}"), genome_name=GENOME_NAMES, genome_ext=GENOME_EXTS, zip=True)
    output:
        paf_file = os.path.join(OUTPUT_DIR, "{node}.paf")
    params:
        path = CACTUS_GPU_PATH_TMP, # Note that if USE_GPU is False, this will be the same as CACTUS_PATH_TMP
        seq_out = os.path.join(OUTPUT_DIR, "seq_file.out"),
        node = "{node}",
        job_tmp_dir = os.path.join("/tmp", "{node}-blast"), # This is the tmp dir in the container, which is bound to the host tmp dir
        host_tmp_dir = os.path.join(TMPDIR, "{node}-blast"), # This is the tmp dir for the host system, which is bound to /tmp in the singularity container
        gpu_opt = f"--gpu {config['blast_gpu']}" if USE_GPU else "",
        gpu_num = config["blast_gpu"],
        rule_name = "blast"
    log:
        job_log = os.path.join(LOG_DIR, "{node}.blast.log")
    resources:
        slurm_partition = config["blast_partition"],
        cpus_per_task = config["blast_cpu"],
        mem_mb = config["blast_mem"],
        runtime = config["blast_time"],
        slurm_extra = f"'--gres=gpu:{config['blast_gpu']}'" if USE_GPU else ""
    run:
        cmd = params.path + [
            "cactus-blast",
            params.job_tmp_dir,
            params.seq_out,
            output.paf_file,
            "--root", params.node,
            "--logInfo",
            "--retryCount", "0",
            "--lastzCores", str(resources.cpus_per_task)
        ];

        if params.gpu_opt:
            cmd += ["--gpu", str(params.gpu_num)];

        cactuslib.runCommand(cmd, params.host_tmp_dir, log.job_log, params.rule_name, wildcards.node)
## This rule runs cactus-blast for every internal node
## Runtimes for turtles range from 1 to 10 hours with the above resources

# ####################

rule align:
    input:
        paf_file = lambda wildcards: rounds[wildcards.node]['blast-output']
        #paf_file = lambda wildcards: [ rounds[name]['blast-output'] for name in rounds if name == wildcards.node ]
        #paf_file = os.path.join(OUTPUT_DIR, ANCNAME + ".paf")
        #seq_files = lambda wildcards: [ os.path.join(OUTPUT_DIR, input_file) for input_file in internals[wildcards.internal_node]['desc-seqs'] ]
    output:
        hal_file = os.path.join(OUTPUT_DIR, "{node}.hal")
    params:
        path = CACTUS_PATH_TMP,
        cactus_file = os.path.join(OUTPUT_DIR, "seq_file.out"),
        node = "{node}",
        job_tmp_dir = os.path.join("/tmp", "{node}-align"), # This is the tmp dir in the container, which is bound to the host tmp dir
        host_tmp_dir = os.path.join(TMPDIR, "{node}-align"), # This is the tmp dir for the host system, which is bound to /tmp in the singularity container
        work_dir = TMPDIR,
        rule_name = "align"
    log:
        job_log = os.path.join(LOG_DIR, "{node}.align.log")
    resources:
        slurm_partition = config["align_partition"],
        cpus_per_task = config["align_cpu"],
        mem_mb = config["align_mem"],
        runtime = config["align_time"],
    run:
        cmd = params.path + [
            "cactus-align",
            params.job_tmp_dir,
            params.cactus_file,
            input.paf_file,
            output.hal_file,
            "--root", params.node,
            "--logInfo",
            "--retryCount", "0",
            "--maxCores", str(resources.cpus_per_task),
        ];

        cactuslib.runCommand(cmd, params.host_tmp_dir, log.job_log, params.rule_name, wildcards.node)
## This rule runs cactus-align for every internal node
## Runtimes for turtles range from 4 to 16 hours with the above resources

####################

rule convert:
    input:
        hal_file = lambda wildcards: rounds[wildcards.node]['align-output']
        #lambda wildcards: [ os.path.join(output_dir, input_file) for input_file in internals[wildcards.internal_node]['hal-inputs'] ][0]
    output:
        fa_file = os.path.join(OUTPUT_DIR, "{node}.fa")
    params:
        path = CACTUS_PATH,
        node = "{node}",
        rule_name = "convert"
    log:
        job_log = os.path.join(LOG_DIR, "{node}.convert.log")
    resources:
        slurm_partition = config["convert_partition"],
        cpus_per_task = config["convert_cpu"],
        mem_mb = config["convert_mem"],
        time = config["convert_time"]
    run:
        cmd = params.path + [
            "hal2fasta",
            input.hal_file,
            params.node,
            "--outFaPath", output.fa_file,
            "--hdf5InMemory"
        ];

        cactuslib.runCommand(cmd, None, log.job_log, params.rule_name, params.node)
## This rule runs hal2fasta to convert .hal files for each internal node to .fasta files
## Runtime for turtles is only about 30 seconds per node

####################

rule copy_or_get_hal:
    input:
        input_hal = INPUT_HAL
    output:
        stamp = os.path.join(LOG_DIR, "copy-hal.stamp")
    params:
        overwrite_original_hal = config["overwrite_original_hal"],
        hal_to_edit = HAL_TO_EDIT,
        rule_name = "copy_or_get_hal"
    log:
        job_log = os.path.join(LOG_DIR, "copy-hal.log")
    resources:
        slurm_partition = config["copy_or_get_hal_partition"],
        cpus_per_task = config["copy_or_get_hal_cpu"],
        mem_mb = config["copy_or_get_hal_mem"],
        runtime = config["copy_or_get_hal_time"]    
    run:
        if not params.overwrite_original_hal:
            # If the hal file is not being overwritten, copy the input hal file to the output directory
            cmd = ["cp", input.input_hal, params.hal_to_edit];
            cactuslib.runCommand(cmd, None, log.job_log, params.rule_name)

            with open(output.stamp, "w") as f:
                f.write(f"original hal copied: {params.hal_to_edit}");
        else:
            with open(output.stamp, "w") as f:
                f.write(f"using original hal: {params.hal_to_edit}");
## Copying the root .hal file here, since failures in the subsequent rules
## would mean the blast/align steps have to be re-run for that node, but this means a little extra
## storage is required

# ####################

rule add_to_branch:
    input:
        expand(os.path.join(OUTPUT_DIR, "{node}.fa"), node=rounds.keys()),
        copy_hal_stamp = os.path.join(LOG_DIR, "copy-hal.stamp")
    output:
        stamp = os.path.join(LOG_DIR, "add_to_branch.stamp")
    params:
        path = CACTUS_PATH,
        hal_to_edit = HAL_TO_EDIT,
        anc_hal = os.path.join(OUTPUT_DIR, ANCNAME + ".hal"),
        parent_hal = os.path.join(OUTPUT_DIR, PARENT + ".hal"),
        parent = PARENT,
        anc_name = ANCNAME,
        child = CHILD,
        genome_name = GENOME_NAME,
        top_bl = TOP_BL,
        orig_bl = ORIG_BL,
        host_tmp_dir = os.path.join(TMPDIR, "add-to-branch"),
        rule_name = "add_to_branch"
    log:
        job_log = os.path.join(LOG_DIR, "hal-add-to-branch.log")
    resources:
        slurm_partition = config["add_to_branch_partition"],
        cpus_per_task = config["add_to_branch_cpu"],
        mem_mb = config["add_to_branch_mem"],
        runtime = config["add_to_branch_time"]
    run:
        cmd = params.path + [
            "halAddToBranch",
            params.hal_to_edit,
            params.anc_hal,
            params.parent_hal,
            params.parent,
            params.anc_name,
            params.child,
            params.genome_name,
            params.top_bl,
            params.orig_bl,
            "--hdf5InMemory"
        ];

        cactuslib.runCommand(cmd, params.host_tmp_dir, log.job_log, params.rule_name)

        with open(output.stamp, "w") as f:
            f.write("done");
        # This is just a stamp file to indicate that the rule has completed

      ## This rule runs halAddToBranch to add the new genome to the tree
####################

rule maf:
    input:
        stamp = os.path.join(LOG_DIR, "add_to_branch.stamp")
    output:
        final_maf = OUTPUT_MAF,
        final_maf_nodupes = OUTPUT_MAF_NODUPES
    params:
        path = CACTUS_PATH_TMP,
        hal_to_edit = HAL_TO_EDIT,
        ref_genome = MAF_REFERENCE,
        chunk_size = 500000, # 500kb
        host_tmp_dir = os.path.join(TMPDIR, "maf"),
        job_tmp_dir = os.path.join("/tmp", "maf"),
        rule_name = "maf"
    log:
        job_log = os.path.join(LOG_DIR, "maf.log")
    resources:
        slurm_partition = config["maf_partition"],
        cpus_per_task = config["maf_cpu"],
        mem_mb = config["maf_mem"],
        runtime = config["maf_time"]
    run:
        cmd = params.path + [
            "cactus-hal2maf",
            params.job_tmp_dir,
            params.hal_to_edit,
            output.final_maf,
            "--refGenome", params.ref_genome,
            "--chunkSize", str(params.chunk_size),
            "--batchCount", str(resources.cpus_per_task),
            "--filterGapCausingDupes"
        ];

        cactuslib.runCommand(cmd, params.job_tmp_dir, log.job_log, params.rule_name);

        cmd = params.path + [
            "cactus-hal2maf",
            params.job_tmp_dir,
            params.hal_to_edit,
            output.final_maf_nodupes,
            "--refGenome", params.ref_genome,
            "--chunkSize", str(params.chunk_size),
            "--batchCount", str(resources.cpus_per_task),
            "--filterGapCausingDupes",
            "--dupeMode", "single"
        ];

        cactuslib.runCommand(cmd, params.host_tmp_dir, log.job_log, params.rule_name, fmode="a+");

#############################################################################
